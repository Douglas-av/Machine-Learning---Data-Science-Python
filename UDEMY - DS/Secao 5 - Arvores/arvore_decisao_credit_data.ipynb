{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lendo as bases de treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Bases/credit_data.pkl', 'rb') as file:\n",
    "    X_credit_treinamento, X_credit_teste, y_credit_treinamento, y_credit_teste = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1500, 3), (1500,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_credit_treinamento.shape, y_credit_treinamento.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((500, 3), (500,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_credit_teste.shape, y_credit_teste.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arvore de decisao - 98,2%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy', random_state=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arvore_credit_data = DecisionTreeClassifier(criterion='entropy', random_state=0)\n",
    "arvore_credit_data.fit(X_credit_treinamento, y_credit_treinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsoes = arvore_credit_data.predict(X_credit_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.982"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_credit_teste, previsoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[430,   6],\n",
       "       [  3,  61]], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_credit_teste, previsoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.classifier import ConfusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.982"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = ConfusionMatrix(arvore_credit_data)\n",
    "cm.fit(X_credit_treinamento, y_credit_treinamento)\n",
    "cm.score(X_credit_teste, y_credit_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       436\n",
      "           1       0.91      0.95      0.93        64\n",
      "\n",
      "    accuracy                           0.98       500\n",
      "   macro avg       0.95      0.97      0.96       500\n",
      "weighted avg       0.98      0.98      0.98       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_credit_teste, previsoes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5109d816b82be14675a6b11f8e0f0d2e80f029176ed3710d54e125caa8520dfd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
